This project trains an MAE on ECG images that have been generated from publicly available time series data to extract features for downstream classification. The MAE will provide the basis for a foundation model. The MAE framework comes from mmpretrain (https://mmpretrain.readthedocs.io/en/dev/papers/mae.html).

The data involved in this project is from 5 sources below:

Public data:
- Physionet (... images)

Clinical data:
- PRECISE (... images)
- Huawei (... images)
- L-HARP (... images)
- Guangzhou (... images)

The public data will be used for training the MAE and the clinical data for the downstream classification task. The PhysioNet data came from the Computing in Cardiology 2021 challenge [M. A. Reyna et al., "Will Two Do? Varying Dimensions in Electrocardiography: The PhysioNet/Computing in Cardiology Challenge 2021," 2021 Computing in Cardiology (CinC), Brno, Czech Republic, 2021, pp. 1-4, doi: 10.23919/CinC53138.2021.9662687.]. This data was time series collected from 7 different sources, with 88,000 time series ECGs publicly available. Within this project, these time series ECGs were converted to realistic scanned images using ecg-plot and that was used as input for the MAE. When converting the physionet data from time series to images, multiple images were able to be generated from a singular time series due to extended time length, hence training of the MAE was performed on ... images. 

The clinical data for downstream classification came from numerous different sources that are not publicly available. They all have different outcomes of interest. However the clinical data differs from the public data since they are real scans, there is noise and artefacts present making them harder to classify. 

<p align="center">
  <figure style="display:inline-block">
    <img src="https://github.com/user-attachments/assets/cb07d7d3-9d67-4e27-aaf4-a3a9c2290ffc" alt="Example of generated scanned image from PhysioNet data." width="45%">
    <figcaption>Example of generated scanned image from PhysioNet data.</figcaption>
  </figure>
  <figure style="display:inline-block">
    <img src="https://github.com/user-attachments/assets/5d09b567-7dc0-4dc2-ae4a-db548889a988" alt="Example of L-HARP scan." width="45%">
    <figcaption>Example of L-HARP scan.</figcaption>
  </figure>
</p>


1. Create conda environment for MAE training and install mmpretrain:

```
# Create conda environment
conda create --name openmmlab python=3.8 -y

# Activate conda environment
source activate openmmlab

# Install pytorch
pip3 install torch==1.8.2 torchvision==0.9.2 torchaudio==0.8.2 --extra-index-url https://download.pytorch.org/whl/lts/1.8/cu111

# Follow the instructions at Prerequisites â€” MMPretrain 1.2.0 documentation, 
git clone https://github.com/open-mmlab/mmpretrain.git
cd mmpretrain
pip install -U openmim && mim install -e .

# Test installation
python demo/image_demo.py demo/demo.JPEG resnet18_8xb32_in1k --device cpu
python demo/image_demo.py demo/demo.JPEG resnet18_8xb32_in1k --device gpu
```

2. Prepare dataset and config files

To use a custom dataset, files need to be organised either in (https://mmpretrain.readthedocs.io/en/dev/user_guides/dataset_prepare.html):
- Subfolder format
- Text annotation file format

Then adjust associated configuration files (https://mmpretrain.readthedocs.io/en/dev/user_guides/config.html). There are 4 types:
- Models
- Datasets
- Schedules
- Runtime

From the mmpretrain GitHub repository, you can select a model (https://github.com/open-mmlab/mmpretrain). For this application, the self-supervised MAE (CVPR'2022) was chosen. The GitHub repository contains all the configurations files however these will have been downloaded when installing mmpretrain in Step 1 (https://github.com/open-mmlab/mmpretrain/tree/main/configs/mae).

In this repository, the folder 'my_configs/training' contains the configurations used for this project training of MAE. 

3. To train the MAE on dataset run in command line; **be mindful that wherever you execute this command, that is where the work_dirs will be saved e.g. output**:

```
# Run training
python tools/train.py /my_configs/training/mae_vit-base-16_8xb512-amp-coslr-300e_in1k.py
```

Output will be a folder 'work_dirs' containing:
- epoch.pth files
- last checkpoint file
- folder containing
  - log
  - vis_data folder
- config.py file

4. To test on physionet data, config files are contained in this repository in 'my_configs/testing'.

```
# Create new folder to save test results
mkdir work_dirs_test
cd work_dirs_test

# Run test 
python tools/test.py /my_configs/testing/classification.py /work_dirs/checkpoint.pth
```

5. To test on Guangzhou data...

